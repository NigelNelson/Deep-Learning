{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Training and testing with the from-scratch library\n",
    "### Author: Nigel Nelson\n",
    "### Course: CS-3450\n",
    "## Introduction:\n",
    "- This lab builds upon past labs, where the forward and backward propagation operations were defined for a set of network layers. This lab combines this past work to build a simple neural network that is then trained on the fashion-MNIST data set in order to prove that the underlying mathematical functions are operating correctly.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network\n",
    "import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "warnings.filterwarnings('ignore')  # If you see warnings that you know you can ignore, it can be useful to enable this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fashion-MNIST and similar problems\n",
    "DATA_ROOT = '/data/cs3450/data/'\n",
    "FASHION_MNIST_TRAINING = '/data/cs3450/data/fashion_mnist_flattened_training.npz'\n",
    "FASHION_MNIST_TESTING = '/data/cs3450/data/fashion_mnist_flattened_testing.npz'\n",
    "CIFAR10_TRAINING = '/data/cs3450/data/cifar10_flattened_training.npz'\n",
    "CIFAR10_TESTING = '/data/cs3450/data/cifar10_flattened_testing.npz'\n",
    "CIFAR100_TRAINING = '/data/cs3450/data/cifar100_flattened_training.npz'\n",
    "CIFAR100_TESTING = '/data/cs3450/data/cifar100_flattened_testing.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# With this block, we don't need to set device=DEVICE for every tensor.\n",
    "torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "     torch.cuda.set_device(0)\n",
    "     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "     print(\"Running on the GPU\")\n",
    "else:\n",
    "     print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is a square example\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patterns():\n",
    "    \"\"\"\n",
    "    I don't remember what sort of data this generates -- Dr. Yoder\n",
    "\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    pattern1 = torch.tensor([[1, 0, 1, 0, 1, 0]],dtype=torch.float32).T\n",
    "    pattern2 = torch.tensor([[1, 1, 1, 0, 0, 0]],dtype=torch.float32).T\n",
    "    num_samples = 1000\n",
    "\n",
    "    x = torch.zeros((pattern1.shape[0],num_samples))\n",
    "    y = torch.zeros((2,num_samples))\n",
    "    # TODO: Implement with shuffling instead?\n",
    "    for i in range(0,num_samples):\n",
    "        if torch.rand(1) > 0.5:\n",
    "            x[:,i:i+1] = pattern1\n",
    "            y[:,i:i+1] = torch.tensor([[0,1]],dtype=torch.float32).T\n",
    "        else:\n",
    "            x[:,i:i+1] = pattern2\n",
    "            y[:,i:i+1] = torch.tensor([[1,0]],dtype=torch.float32).T\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_flattened(train=True,dataset='Fashion-MNIST',download=False):\n",
    "    \"\"\"\n",
    "    :param train: True for training, False for testing\n",
    "    :param dataset: 'Fashion-MNIST', 'CIFAR-10', or 'CIFAR-100'\n",
    "    :param download: True to download. Keep to false afterwords to avoid unneeded downloads.\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    if dataset == 'Fashion-MNIST':\n",
    "        if train:\n",
    "            path = FASHION_MNIST_TRAINING\n",
    "        else:\n",
    "            path = FASHION_MNIST_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-10':\n",
    "        if train:\n",
    "            path = CIFAR10_TRAINING\n",
    "        else:\n",
    "            path = CIFAR10_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-100':\n",
    "        if train:\n",
    "            path = CIFAR100_TRAINING\n",
    "        else:\n",
    "            path = CIFAR100_TESTING\n",
    "        num_labels = 100\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: '+str(dataset))\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading cached flattened data for',dataset,'training' if train else 'testing')\n",
    "        data = np.load(path)\n",
    "        x = torch.tensor(data['x'],dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'],dtype=torch.float32)\n",
    "        pass\n",
    "    else:\n",
    "        class ToTorch(object):\n",
    "            \"\"\"Like ToTensor, only redefined by us for 'historical reasons'\"\"\"\n",
    "\n",
    "            def __call__(self, pic):\n",
    "                return torchvision.transforms.functional.to_tensor(pic)\n",
    "\n",
    "        if dataset == 'Fashion-MNIST':\n",
    "            data = torchvision.datasets.FashionMNIST(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            data = torchvision.datasets.CIFAR10(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-100':\n",
    "            data = torchvision.datasets.CIFAR100(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        else:\n",
    "            raise ValueError('This code should be unreachable because of a previous check.')\n",
    "        x = torch.zeros((len(data[0][0].flatten()), len(data)),dtype=torch.float32)\n",
    "        for index, image in enumerate(data):\n",
    "            x[:, index] = data[index][0].flatten()\n",
    "        labels = torch.tensor([sample[1] for sample in data])\n",
    "        y = torch.zeros((num_labels, len(labels)), dtype=torch.float32)\n",
    "        y[labels, torch.arange(len(labels))] = 1\n",
    "        np.savez(path, x=x.numpy(), y=y.numpy())\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_net_forward():\n",
    "    \"\"\"\n",
    "    Function used to verify that the forward propagation of the network works correctly\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu:0')\n",
    "    dtype = torch.float64\n",
    "\n",
    "    x = torch.tensor([[3], [2]], dtype=dtype, device=device) \n",
    "    W = torch.tensor([[4, 5], [-2, 2], [7, 1]], dtype=dtype, device=device)\n",
    "    b1 = torch.tensor([[1], [-2], [3]], dtype=dtype, device=device)\n",
    "    M = torch.tensor([[-4, 5, 3], [-2, 2, 7]], dtype=dtype, device=device)\n",
    "    b2 = torch.tensor([[-3], [2]], dtype=dtype, device=device)\n",
    "\n",
    "    x_layer = layers.Input((2,1))\n",
    "    \n",
    "    W_layer = layers.Input((3,2))\n",
    "    W_layer.set(W)\n",
    "    b1_layer = layers.Input((3,1))\n",
    "    b1_layer.set(b1)\n",
    "    M_layer = layers.Input((2,3))\n",
    "    M_layer.set(M)\n",
    "    b2_layer = layers.Input((2,1))\n",
    "    b2_layer.set(b2)\n",
    "    \n",
    "    linear1 = layers.Linear(x_layer, W_layer, b1_layer)\n",
    "    relu = layers.ReLU(linear1)\n",
    "    linear2 = layers.Linear(relu, M_layer, b2_layer)\n",
    "    \n",
    "    net = network.Network()\n",
    "    \n",
    "    net.add(x_layer)\n",
    "    net.add(linear1)\n",
    "    net.add(relu)\n",
    "    net.add(linear2)\n",
    "    \n",
    "    net.forward(x)\n",
    "    \n",
    "    net.layers[-1].accumulate_grad(torch.tensor([[1], [1]], dtype=torch.float64))\n",
    "    net.backward()\n",
    "    \n",
    "    print('The expected output is:')\n",
    "    print(torch.tensor([[-17], [138]]))\n",
    "    print()\n",
    "    print('The actual output is:')\n",
    "    print(net.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = 'Fashion-MNIST'\n",
    "    # dataset = 'CIFAR-10'\n",
    "    # dataset = 'CIFAR-100'\n",
    "\n",
    "#     x_train, y_train = create_linear_training_data()\n",
    "#     x_train, y_train = create_folded_training_data()\n",
    "#     points_train, insideness_train = create_square()\n",
    "    x_train, y_train = load_dataset_flattened(train=True, dataset=dataset, download=False)\n",
    "    x_test, y_test = load_dataset_flattened(train=False, dataset=dataset, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "num_hidden_nodes = 24\n",
    "learing_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "x_layer = layers.Input((x_train.shape[0], batch_size), train=False)\n",
    "W_layer = layers.Input((num_hidden_nodes, x_train.shape[0]))\n",
    "W_layer.randomize()\n",
    "b1_layer = layers.Input((num_hidden_nodes, batch_size))\n",
    "b1_layer.randomize()\n",
    "M_layer = layers.Input((y_train.shape[0], num_hidden_nodes))\n",
    "M_layer.randomize()\n",
    "b2_layer = layers.Input((y_train.shape[0], batch_size))\n",
    "b2_layer.randomize()\n",
    "\n",
    "# Scale the weight matrices\n",
    "W_layer.output.data *= 0.01\n",
    "M_layer.output.data *= 0.01\n",
    "\n",
    "#define meta layers\n",
    "linear1 = layers.Linear(x_layer, W_layer, b1_layer)\n",
    "relu = layers.ReLU(linear1)\n",
    "linear2 = layers.Linear(relu, M_layer, b2_layer)\n",
    "y_layer = layers.Input((y_train.shape[0], batch_size), train=False)\n",
    "softmax = layers.Softmax(y_layer, linear2)\n",
    "\n",
    "# Intialize the network and add its layers in order of execution\n",
    "net = network.Network()\n",
    "net.add(x_layer)\n",
    "net.add(W_layer)\n",
    "net.add(b1_layer)\n",
    "net.add(linear1)\n",
    "net.add(relu)\n",
    "net.add(M_layer)\n",
    "net.add(b2_layer)\n",
    "net.add(linear2)\n",
    "net.add(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_metrics_dict = {\n",
    "    'training_acc': [],\n",
    "    'testing_acc': [],\n",
    "    'training_loss': [],\n",
    "    'testing_loss': [],\n",
    "}\n",
    "      \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Initialize training metrics to 0\n",
    "    training_num_correct = 0\n",
    "    total_training_loss = 0\n",
    "    testing_num_correct = 0\n",
    "    total_testing_loss = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for i in range(x_train.shape[1]//batch_size):\n",
    "        \n",
    "        # Get the correct locations to reference in the training set\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = i*batch_size + batch_size\n",
    "        \n",
    "        complete_true_labels = y_train[:, start_idx : end_idx].reshape(y_train.shape[0], batch_size)\n",
    "\n",
    "        net.layers[-1].actual.set(complete_true_labels)\n",
    "        \n",
    "        input_data = x_train[:, start_idx : end_idx].reshape(x_train.shape[0], batch_size)\n",
    "\n",
    "        net.forward(input_data)\n",
    "\n",
    "        # Collect the loss from the training\n",
    "        total_training_loss += net.layers[len(net.layers) - 1].output\n",
    "\n",
    "        # Collect the predicted values \n",
    "        predictions = torch.argmax(net.layers[len(net.layers) - 1].softmax(), dim=0)\n",
    "        true_labels = torch.argmax(y_train[:, start_idx : end_idx].reshape(y_train.shape[0], batch_size), dim=0)\n",
    "  \n",
    "        training_num_correct += (predictions == true_labels).sum() \n",
    "\n",
    "        net.backward()\n",
    "        net.step(learing_rate)\n",
    "       \n",
    "    #Testing loop\n",
    "    for i in range(x_test.shape[1]//batch_size):\n",
    "        \n",
    "        # Get the correct locations to reference in the testing set\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = i*batch_size + batch_size\n",
    "        \n",
    "        complete_true_labels = y_test[:, start_idx : end_idx].reshape(y_test.shape[0], batch_size)\n",
    "\n",
    "        net.layers[-1].actual.set(complete_true_labels)\n",
    "        \n",
    "        input_data = x_test[:, start_idx : end_idx].reshape(x_test.shape[0], batch_size)\n",
    "\n",
    "        net.forward(input_data)\n",
    "\n",
    "        total_testing_loss += net.layers[len(net.layers) - 1].output\n",
    "\n",
    "        predictions = torch.argmax(net.layers[len(net.layers) - 1].softmax(), dim=0)\n",
    "        true_labels = torch.argmax(y_test[:, start_idx : end_idx].reshape(y_test.shape[0], batch_size), dim=0)\n",
    "  \n",
    "        testing_num_correct += (predictions == true_labels).sum() \n",
    "       \n",
    "        \n",
    "    # Calculate all training/testing metrics and print to console for each epoch    \n",
    "    training_loss = total_training_loss.item()/y_train.shape[1]\n",
    "    testing_loss = total_testing_loss.item()/y_test.shape[1]\n",
    "    training_acc = training_num_correct/y_train.shape[1]\n",
    "    testing_acc = testing_num_correct/y_test.shape[1]\n",
    "\n",
    "    print(f'Epoch #{epoch + 1}:')\n",
    "    print(f'\\tTraining Loss: {training_loss}')\n",
    "    print(f'\\tTesting Loss: {testing_loss}')\n",
    "    print(f'\\tTraining accuracy: {training_acc}')\n",
    "    print(f'\\tTesting accuracy: {testing_acc}')\n",
    "    print()\n",
    "    \n",
    "    epoch_metrics_dict['training_loss'].append(training_loss)\n",
    "    epoch_metrics_dict['testing_loss'].append(testing_loss)\n",
    "    epoch_metrics_dict['training_acc'].append(training_acc)\n",
    "    epoch_metrics_dict['testing_acc'].append(testing_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Curves for Training and Testing Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epochs), epoch_metrics_dict['training_loss'], label='training')\n",
    "plt.plot(range(num_epochs), epoch_metrics_dict['testing_loss'], label='testing')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Cross Entropy Loss vs. Epoch #')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epochs), epoch_metrics_dict['training_acc'], label='training')\n",
    "plt.plot(range(num_epochs), epoch_metrics_dict['testing_acc'], label='testing')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Epoch #')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Summary of Training Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||Accuracy|Loss (Cross-Entropy)|\n",
    "|------|------|--------|\n",
    "|Training|0.8547|1.6946|\n",
    "|Testing|0.8423|1.6865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "- This lab sequence has severed as a deep dive in neural networks, and the underlying mathematical functions that allow them to model the data that they are trained on. The first portion of this lab was deriving and testing the forward propagation of our network layers. This was a fairly straight forward experience, yet one of my biggest take-aways from this phase was the importance of verifying the shapes of the matrices that are being operated on. This is because most of the issues in my network were discovered once I realized a pair of matrices did not have the correct shapes. Following this, the back propagation operations for our network layers needed to be derived. From this experience, I re-learned the chain rule for partial derivatives, and through practice, ultimately developed a methodology to derive the derivatives of any network layer. Thanks to the experience I gained working the mathematical operations of the network by hand, creating unit tests for the network and its respective layers was fairly straight forward. With a true understanding of the operations applied at each layer, it was trivial to choose random numbers, work out by hand the expected answers, then use those expected answers in assertions to ensure the layers were behaving properly. Overall, the most frustrating portion of this process was definitely debugging a network that wouldn’t train. This is due in large part to the fact that Jupyter notebooks do not have a debugging environment, and as such when the network wasn't training, one had to take educated guesses and simply tamper with the parameters and operations until they saw a change in the outputs. The issues that I ran into resulted in disappearing gradients where my network would no longer train. The first cause was the fact that I had too many hidden nodes, 256 to be exact. When I would attempt to train with this many hidden nodes, my network would train for a single epoch, and then have the gradients diminish to 0 and the loss and accuracy would not change for the remainder of the training session. What I believe happened here is that with a larger batch size, the inner matrices for these hidden nodes were so large that each parameter’s partial derivative was such a small fraction of the final loss, that those derivatives were essentially 0, resulting in no change to the parameters epoch to epoch. The second issue that I ran into was that without multiplying my weight vectors by 0.1 or 0.01, my gradients would drop to 0 after a couple epochs. The reason that I believe this step was necessary is that with larger weights in layers with many hidden nodes, the resulting matrix multiplications result in huge numbers, which in turn result in a large loss. This is an issue as subtracting each parameter's partial derivative with respect to a massive loss can in turn cause the existing weights to be wiped out, causing the weight matrices to lose their gradient. This was a difficult issue to debug, but ultimately by scaling the initialization values of the weight matrices, the issue of disappearing gradients was ultimately resolved\n",
    "- Through all of this knowledge gained, I was ultimately able to get a testing accuracy of **0.846** for the Fashion-MNIST data set. This was done with 24 hidden nodes, a batch size of 4, and a learning rate of 0.001. In order to decrease the likely-hood of overfitting, I trained for only 10 epochs, which is less than half of what I had trained for in earlier lab sequences. Thanks to this approach, my training curves tend to show that there is minimal overfitting occurring in my training. This can be seen in both the loss and accuracy curves, where the training values begin terribly, reflecting the random initialization of weights, but by the 1st epoch achieve respectable scores. From that first epoch on, the training and testing curves remain reasonably entangled, without one showing drastic increases or decreases that is not mirrored by the other. As such, this is an indication that my network generalized the underlying patterns of the data set with almost 85% accuracy, and minimally memorized the training data set."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
