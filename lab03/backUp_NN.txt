num_hidden_nodes = 2

W = torch.rand((num_hidden_nodes,2), dtype=torch.float32, device=device, requires_grad=True)
W.data *= 0.1
b1 = torch.zeros((num_hidden_nodes,1), dtype=torch.float32, device=device, requires_grad=True)

M = torch.rand((2,num_hidden_nodes), dtype=torch.float32, device=device, requires_grad=True)
M.data *= 0.1
b2 = torch.zeros((2,1), dtype=torch.float32, device=device, requires_grad=True)

W_layer = Input((num_hidden_nodes, 2))
W_layer.set(W)
b1_layer = Input((num_hidden_nodes,1))
b1_layer.set(b1)

M_layer = Input((2,num_hidden_nodes))
M_layer.set(M)
b2_layer = Input((2,1))
b2_layer.set(b2)

num_epochs = 500
learning_rate = .01
reg_const = 0.000001
batch_size = 8

x1_layer = Input((x_train.shape[0], batch_size))
linear_layer1 = LinearReLU(x1_layer, W_layer, b1_layer)

x2_layer = Input((b1_layer.output.shape[0], batch_size))
linear_layer2 = Linear(x2_layer, M_layer, b2_layer)


for epoch in range(num_epochs):
#     learning_rate /= 10**epoch
#     print(learning_rate)
    print(f'epoch #{epoch}')
    for i in range(x_train.shape[1]//batch_size):
        x1_layer.set(x_train[:, i*batch_size : (i*batch_size + batch_size)].reshape(x_train.shape[0], batch_size))

        linear_layer1.forward()

        x2_layer.set(linear_layer1.output)

        linear_layer2.forward()
        
        #(1/linear_layer2.output.numel()) * 
        
        mse = MSE((y_train[:,i*batch_size : (i*batch_size + batch_size)]).reshape(y_train.shape[0], batch_size),
                 linear_layer2.output)

        #mse = (((y_train[:,i*batch_size : (i*batch_size + batch_size)]).reshape(y_train.shape[0], batch_size) - linear_layer2.output)**2).sum()
        
        s1 = (W_layer.output**2).sum()
        s2 = (M_layer.output**2).sum()
        S = reg_const*(s1 + s2)
        
        cost = mse + S

        cost.backward()

        with torch.no_grad():

            W_layer.output -= learning_rate * W_layer.output.grad
            b1_layer.output -= learning_rate * b1_layer.output.grad

            M_layer.output -= learning_rate * M_layer.output.grad
            b2_layer.output -= learning_rate * b2_layer.output.grad

            W_layer.output.grad.zero_()
            b1_layer.output.grad.zero_()
            M_layer.output.grad.zero_()
            b2_layer.output.grad.zero_()
            
    print(mse.item())