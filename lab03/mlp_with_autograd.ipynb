{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Training a Network Using PyTorch's Autograd\n",
    "### Author: Nigel Nelson\n",
    "### Course: CS-3450\n",
    "### Date: 3/25/2022\n",
    "---\n",
    "---\n",
    "## Computing Forward\n",
    "\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import warnings\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_const = 0.01\n",
    "x = torch.tensor([[0], [1]])\n",
    "W = torch.tensor([[1, 1], [1, 1], [0, 0]])\n",
    "b1 = torch.tensor([[0], [1], [1]])\n",
    "M = torch.tensor([[1, 0, 1], [1, 0, 1]])\n",
    "b2 = torch.tensor([[1], [0]])\n",
    "y = torch.tensor([[2], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V term output:\n",
      "tensor([[3],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "u = W @ x + b1\n",
    "h = ReLU(u)\n",
    "v = M @ h + b2\n",
    "print('V term output:')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 loss output:\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "L = (1/y.numel())*((y - v)**2).sum()\n",
    "print('L2 loss output:')\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization term output:\n",
      "tensor(0.0800)\n"
     ]
    }
   ],
   "source": [
    "s1 = (W**2).sum()\n",
    "s2 = (M**2).sum()\n",
    "s = reg_const * (s1 + s2)\n",
    "print('Standardization term output:')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output of the network:\n",
      "tensor(0.5800)\n"
     ]
    }
   ],
   "source": [
    "J = L + s\n",
    "print('Final output of the network:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_const = 0.01\n",
    "x = torch.tensor([[2], [3]])\n",
    "W = torch.tensor([[3, 1], [2, 4], [0, 2]])\n",
    "b1 = torch.tensor([[2], [-2], [1]])\n",
    "M = torch.tensor([[-2, 0, 3], [3, 2, 3]])\n",
    "b2 = torch.tensor([[-1], [2]])\n",
    "y = torch.tensor([[1], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V term output:\n",
      "tensor([[-2],\n",
      "        [84]])\n"
     ]
    }
   ],
   "source": [
    "u = W @ x + b1\n",
    "h = ReLU(u)\n",
    "v = M @ h + b2\n",
    "print('V term output:')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 loss output:\n",
      "tensor(3366.5000)\n"
     ]
    }
   ],
   "source": [
    "L = (1/y.numel())*((y - v)**2).sum()\n",
    "print('L2 loss output:')\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization term output:\n",
      "tensor(0.6900)\n"
     ]
    }
   ],
   "source": [
    "s1 = (W**2).sum()\n",
    "s2 = (M**2).sum()\n",
    "s = reg_const * (s1 + s2)\n",
    "print('Standardization term output:')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output of the network:\n",
      "tensor(3367.1899)\n"
     ]
    }
   ],
   "source": [
    "J = L + s\n",
    "print('Final output of the network:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_const = 0.01\n",
    "x = torch.tensor([[-3], [4]])\n",
    "W = torch.tensor([[-1, 3], [3, 2], [4, -2]])\n",
    "b1 = torch.tensor([[-3], [4], [0]])\n",
    "M = torch.tensor([[3, 2, 1], [-1, -2, -3]])\n",
    "b2 = torch.tensor([[5], [4]])\n",
    "y = torch.tensor([[4], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V term output:\n",
      "tensor([[ 47],\n",
      "        [-14]])\n"
     ]
    }
   ],
   "source": [
    "u = W @ x + b1\n",
    "h = ReLU(u)\n",
    "v = M @ h + b2\n",
    "print('V term output:')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 loss output:\n",
      "tensor(1052.5000)\n"
     ]
    }
   ],
   "source": [
    "L = (1/y.numel())*((y - v)**2).sum()\n",
    "print('L2 loss output:')\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization term output:\n",
      "tensor(0.7100)\n"
     ]
    }
   ],
   "source": [
    "s1 = (W**2).sum()\n",
    "s2 = (M**2).sum()\n",
    "s = reg_const * (s1 + s2)\n",
    "print('Standardization term output:')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output of the network:\n",
      "tensor(1053.2100)\n"
     ]
    }
   ],
   "source": [
    "J = L + s\n",
    "print('Final output of the network:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Provided Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000\n",
    "\n",
    "# For fashion-MNIST and similar problems\n",
    "DATA_ROOT = '/../../data/cs3450/data/'\n",
    "FASHION_MNIST_TRAINING = '/../../data/cs3450/data/fashion_mnist_flattened_training.npz'\n",
    "FASHION_MNIST_TESTING = '/../../data/cs3450/data/fashion_mnist_flattened_testing.npz'\n",
    "CIFAR10_TRAINING = '/../../data/cs3450/data/cifar10_flattened_training.npz'\n",
    "CIFAR10_TESTING = '/../../data/cs3450/data/cifar10_flattened_testing.npz'\n",
    "CIFAR100_TRAINING = '/../../data/cs3450/data/cifar100_flattened_training.npz'\n",
    "CIFAR100_TESTING = '/../../data/cs3450/data/cifar100_flattened_testing.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
    "       https://d2l.ai/chapter_deep-learning-computation/use-gpu.html\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "DEVICE=try_gpu()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is the square example that we looked at in class.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset_flattened(train=True,dataset='Fashion-MNIST',download=False):\n",
    "    \"\"\"\n",
    "    :param train: True for training, False for testing\n",
    "    :param dataset: 'Fashion-MNIST', 'CIFAR-10', or 'CIFAR-100'\n",
    "    :param download: True to download. Keep to false afterwords to avoid unneeded downloads.\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    if dataset == 'Fashion-MNIST':\n",
    "        if train:\n",
    "            path = FASHION_MNIST_TRAINING\n",
    "        else:\n",
    "            path = FASHION_MNIST_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-10':\n",
    "        if train:\n",
    "            path = CIFAR10_TRAINING\n",
    "        else:\n",
    "            path = CIFAR10_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-100':\n",
    "        if train:\n",
    "            path = CIFAR100_TRAINING\n",
    "        else:\n",
    "            path = CIFAR100_TESTING\n",
    "        num_labels = 100\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: '+str(dataset))\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading cached flattened data for',dataset,'training' if train else 'testing')\n",
    "        data = np.load(path)\n",
    "        x = torch.tensor(data['x'],dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'],dtype=torch.float32)\n",
    "        pass\n",
    "    else:\n",
    "        class ToTorch(object):\n",
    "            \"\"\"Like ToTensor, only to a numpy array\"\"\"\n",
    "\n",
    "            def __call__(self, pic):\n",
    "                return torchvision.transforms.functional.to_tensor(pic)\n",
    "\n",
    "        if dataset == 'Fashion-MNIST':\n",
    "            data = torchvision.datasets.FashionMNIST(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            data = torchvision.datasets.CIFAR10(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-100':\n",
    "            data = torchvision.datasets.CIFAR100(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        else:\n",
    "            raise ValueError('This code should be unreachable because of a previous check.')\n",
    "        x = torch.zeros((len(data[0][0].flatten()), len(data)),dtype=torch.float32)\n",
    "        for index, image in enumerate(data):\n",
    "            x[:, index] = data[index][0].flatten()\n",
    "        labels = torch.tensor([sample[1] for sample in data])\n",
    "        y = torch.zeros((num_labels, len(labels)), dtype=torch.float32)\n",
    "        y[labels, torch.arange(len(labels))] = 1\n",
    "        np.savez(path, x=x.detach().numpy(), y=y.detach().numpy())\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'Fashion-MNIST'\n",
    "# dataset = 'CIFAR-10'\n",
    "# dataset = 'CIFAR-100'\n",
    "\n",
    "x_train, y_train = create_linear_training_data()\n",
    "#x_train, y_train = create_folded_training_data()\n",
    "#points_train, insideness_train = create_square()\n",
    "#x_train, y_train = load_dataset_flattened(train=True, dataset=dataset, download=False)\n",
    "\n",
    "# Move selected datasets to GPU\n",
    "x_train = x_train.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3149, -2.0681,  0.9812,  ...,  0.2736,  0.3274,  0.2501],\n",
       "        [-0.6397,  0.7172,  0.5234,  ...,  0.2444,  1.2426, -0.5625]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6397, -0.7172, -0.5234,  ..., -0.2444, -1.2426,  0.5625],\n",
       "        [-1.3149, -2.0681,  0.9812,  ...,  0.2736,  0.3274,  0.2501]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test, y_test = create_linear_training_data()\n",
    "# x_test, y_test = load_dataset_flattened(train=False, dataset=dataset, download=False)\n",
    "\n",
    "# Move the selected datasets to the GPU\n",
    "x_test = x_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6147,  0.6495,  0.1535,  ..., -0.8542, -1.3896,  1.4529],\n",
       "        [-0.9008, -0.5556, -0.7908,  ..., -1.2958, -1.3622, -1.9388]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9008,  0.5556,  0.7908,  ...,  1.2958,  1.3622,  1.9388],\n",
       "        [-0.6147,  0.6495,  0.1535,  ..., -0.8542, -1.3896,  1.4529]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Backpropagation with Autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Responsible for modeling a single matrix in an Input\n",
    "    \"\"\"\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        :param output_shape (tuple): the shape of the output array.  When this is a single number,\n",
    "        it gives the number of output neurons. When this is an array, it gives the dimensions \n",
    "        of the array of output neurons.\n",
    "        \"\"\"\n",
    "        if not isinstance(output_shape, tuple):\n",
    "            output_shape = (output_shape,)\n",
    "            \n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        \n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    Responsible for modeling a single matrix in a Linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        :param output_shape (tuple): the shape of the output array. Passed to parent's initializer\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, output_shape)\n",
    "\n",
    "    def set(self, value):\n",
    "        \"\"\"\n",
    "        :param value: Value of the matrix. If the shape of the matrix doesn't meet the expectations\n",
    "        of the Input instance, an assertion error is raised\n",
    "        \"\"\"\n",
    "        assert self.output_shape == value.shape\n",
    "        self.output = value\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearReLU(Layer):\n",
    "    \"\"\"\n",
    "    Class responsible for modeling a Linear Layer with a ReLU activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, x, W, b):\n",
    "        \"\"\"\n",
    "        :param x: The input matrix of the layer\n",
    "        :param W: The weight matrix of the layer\n",
    "        :param b: The biase matrix of the layer. If this doesn't equal the Input's expected shape,\n",
    "        an assertion error is raised\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, b.output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        :param x: The values to perform the ReLU activation function on\n",
    "        \"\"\"\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Sets the layer's output based on the outputs of the layers that feed into it after applying the\n",
    "        ReLU activation function\n",
    "        \"\"\"\n",
    "        self.output = self.ReLU((self.W.output @ self.x.output) + self.b.output)\n",
    "   \n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Class responsible for modeling a Linear Layer without an activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, x, W, b):\n",
    "        \"\"\"\n",
    "        :param x: The input matrix of the layer\n",
    "        :param W: The weight matrix of the layer\n",
    "        :param b: The biase matrix of the layer. If this doesn't equal the Input's expected shape,\n",
    "        an assertion error is raised\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, b.output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Sets the layer's output based on the outputs of the layers that feed into it\n",
    "        \"\"\"\n",
    "        self.output = (self.W.output @ self.x.output) + self.b.output\n",
    "        \n",
    "class Network:\n",
    "    \"\"\"\n",
    "    Class responsible for defining the behavior of a simple Neural Network with a single hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_rows, num_hidden_nodes, dtype=torch.float32, device=torch.device('cuda:0')):\n",
    "        \"\"\"\n",
    "        :param input_rows: The number of rows expected in the input of the network\n",
    "        :param num_hidden_nodes: The number of nodes in the hidden layer desired\n",
    "        :param dtype: The data type to be used with the PyTorch tensors\n",
    "        :param device: The device desired to be used with the PyTorch tensors\n",
    "        \"\"\"\n",
    "        # Define weights and bias matrices for input -> hidden layer\n",
    "        W = torch.rand((num_hidden_nodes, input_rows), dtype=dtype, device=device, requires_grad=True)\n",
    "        W.data *= 0.1\n",
    "        b1 = torch.zeros((num_hidden_nodes,1), dtype=dtype, device=device, requires_grad=True)\n",
    "        \n",
    "        # Define weights and bias matrices for hidden layer -> ouput\n",
    "        M = torch.rand((input_rows ,num_hidden_nodes), dtype=dtype, device=device, requires_grad=True)\n",
    "        M.data *= 0.1\n",
    "        b2 = torch.zeros((input_rows, 1), dtype=dtype, device=device, requires_grad=True)\n",
    "\n",
    "        # Create Input instances for all matrices\n",
    "        W_layer = Input((num_hidden_nodes, input_rows))\n",
    "        W_layer.set(W)\n",
    "        b1_layer = Input((num_hidden_nodes,1))\n",
    "        b1_layer.set(b1)\n",
    "        M_layer = Input((input_rows,num_hidden_nodes))\n",
    "        M_layer.set(M)\n",
    "        b2_layer = Input((input_rows,1))\n",
    "        b2_layer.set(b2)\n",
    "\n",
    "        # Create 1st layer with ReLU activation function\n",
    "        x1_layer = Input(x_train.shape[0])\n",
    "        linear_layer1 = LinearReLU(x1_layer, W_layer, b1_layer)\n",
    "        \n",
    "        # Create 2nd layer without activation function\n",
    "        x2_layer = Input(b1_layer.output.shape[0])\n",
    "        linear_layer2 = Linear(x2_layer, M_layer, b2_layer)\n",
    "        \n",
    "        # Assign class variables\n",
    "        self.layer1 = linear_layer1\n",
    "        self.layer2 = linear_layer2\n",
    "    \n",
    "    \n",
    "    def L2(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Returns the L2 loss of the supplied args\n",
    "        :param actual: The true values\n",
    "        :param predicted: The predicted values\n",
    "        \"\"\"\n",
    "        return ((actual - predicted)**2).mean()\n",
    "        \n",
    "    def train(self, x_train, y_train, num_epochs, learning_rate, reg_const, batch_size):\n",
    "        \"\"\"\n",
    "        Method responsible for training the Neural Network\n",
    "        :param x_train: The X training data\n",
    "        :param y_train: The y training labels\n",
    "        :param num_epochs: Number of epochs to train for\n",
    "        :param learning_rate: The rate at which parameters are adjusted\n",
    "        :param reg_const: The regularization constant that scales the regularization term\n",
    "        :param batch_size: The batch size used for training\n",
    "        \n",
    "        \"\"\"\n",
    "        # Adjust the x matrices according to the batch size\n",
    "        self.layer1.x = Input((x_train.shape[0], batch_size))\n",
    "        self.layer2.x = Input((self.layer1.b.output.shape[0], batch_size))\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(x_train.shape[1]//batch_size):\n",
    "                # Get the correct locations to reference in the training and testing sets\n",
    "                start_idx = i*batch_size\n",
    "                end_idx = i*batch_size + batch_size\n",
    "\n",
    "                # Populate the x matrix with the training samples in this batch\n",
    "                self.layer1.x.set(x_train[:, start_idx : end_idx].reshape(x_train.shape[0], batch_size))\n",
    "                self.layer1.forward()\n",
    "                self.layer2.x.set(self.layer1.output)\n",
    "                self.layer2.forward()\n",
    "\n",
    "                # Calculate the L2 loss using the output of layer 2 and the associated samples in\n",
    "                # y_train\n",
    "                l2 = self.L2((y_train[:, start_idx : end_idx]).reshape(y_train.shape[0], batch_size),\n",
    "                         self.layer2.output)\n",
    "\n",
    "                # Calculate the regularization term\n",
    "                s1 = (self.layer1.W.output**2).sum()\n",
    "                s2 = (self.layer2.W.output**2).sum()\n",
    "                reg = reg_const*(s1 + s2)\n",
    "\n",
    "                # Calculate the final cost term\n",
    "                cost = l2 + reg\n",
    "\n",
    "                # Compute backpropagation with Autograd\n",
    "                cost.backward()\n",
    "\n",
    "                # Used to update parameters inplace\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Adjust parameters according to gradients and the learning rate\n",
    "                    self.layer1.W.output -= learning_rate * self.layer1.W.output.grad\n",
    "                    self.layer1.b.output -= learning_rate * self.layer1.b.output.grad\n",
    "                    self.layer2.W.output -= learning_rate * self.layer2.W.output.grad\n",
    "                    self.layer2.b.output -= learning_rate * self.layer2.b.output.grad\n",
    "\n",
    "                    # Zero the gradients\n",
    "                    self.layer1.W.output.grad.zero_()\n",
    "                    self.layer1.b.output.grad.zero_()\n",
    "                    self.layer2.W.output.grad.zero_()\n",
    "                    self.layer2.b.output.grad.zero_()\n",
    "                    \n",
    "            print(f'Epoch #{epoch + 1} Loss: {l2.item()}')\n",
    "    \n",
    "    def test(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Method responsible for testing the Neural Network after it has been trained\n",
    "        :param x_train: The X training data\n",
    "        :param y_train: The training labels\n",
    "        \"\"\"\n",
    "        # Recalibrate the 1st x layer according to the shape of the testing data\n",
    "        self.layer1.x = Input(x_test.shape)\n",
    "        self.layer1.x.set(x_test)\n",
    "        \n",
    "        self.layer1.forward()\n",
    "        \n",
    "        # Recalibrate the 2nd x layer according to the shape of the output of the 1st layer\n",
    "        self.layer2.x = Input(self.layer1.output.shape)\n",
    "        self.layer2.x.set(self.layer1.output)\n",
    "\n",
    "        self.layer2.forward()\n",
    "\n",
    "        # Computer L2 loss on the expected labels vs. the predicted labels\n",
    "        l2 = self.L2(y_test, self.layer2.output)\n",
    "        print(f'Testing L2 loss: {l2}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network with best parameters found using batches > 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss: 8.001793321454898e-05\n",
      "Epoch #2 Loss: 5.728245378122665e-05\n",
      "Epoch #3 Loss: 9.603369107935578e-06\n",
      "Epoch #4 Loss: 1.3274682260089321e-06\n",
      "Epoch #5 Loss: 8.005353038242902e-07\n",
      "Epoch #6 Loss: 4.996358029529802e-07\n",
      "Epoch #7 Loss: 3.1549438972433563e-07\n",
      "Epoch #8 Loss: 1.919332532906992e-07\n",
      "Epoch #9 Loss: 1.0649807791196508e-07\n",
      "Epoch #10 Loss: 5.0658194084007846e-08\n",
      "Epoch #11 Loss: 1.6401516589326093e-08\n",
      "Epoch #12 Loss: 1.3046591584853218e-09\n",
      "Epoch #13 Loss: 1.463243415322779e-11\n",
      "Epoch #14 Loss: 8.358341796466107e-12\n",
      "Epoch #15 Loss: 6.529582430303549e-12\n",
      "Epoch #16 Loss: 4.785200014012503e-12\n",
      "Epoch #17 Loss: 3.5550729027278294e-12\n",
      "Epoch #18 Loss: 2.7832458560084206e-12\n",
      "Epoch #19 Loss: 1.943029070972102e-12\n",
      "Epoch #20 Loss: 6.072087277431137e-13\n",
      "Epoch #21 Loss: 8.319178679272454e-13\n",
      "Epoch #22 Loss: 2.9012903191016903e-13\n",
      "Epoch #23 Loss: 3.6740055442407993e-13\n",
      "Epoch #24 Loss: 1.7821855102795325e-13\n",
      "Epoch #25 Loss: 6.097899962753672e-14\n",
      "Epoch #26 Loss: 4.232725281383409e-14\n",
      "Epoch #27 Loss: 1.7458257062230587e-14\n",
      "Epoch #28 Loss: 9.739431483524186e-14\n",
      "Epoch #29 Loss: 1.0982881271104361e-13\n",
      "Epoch #30 Loss: 1.301736496372996e-14\n",
      "Epoch #31 Loss: 2.1010970741031088e-14\n",
      "Epoch #32 Loss: 1.5681900222830336e-14\n",
      "Epoch #33 Loss: 1.8346435481930712e-14\n",
      "Epoch #34 Loss: 2.0122792321330962e-14\n",
      "Epoch #35 Loss: 3.877453913503359e-14\n",
      "Epoch #36 Loss: 1.0352829704629585e-14\n",
      "Epoch #37 Loss: 1.69336766830952e-13\n",
      "Epoch #38 Loss: 1.5681900222830336e-14\n",
      "Epoch #39 Loss: 1.9234613901630837e-14\n",
      "Epoch #40 Loss: 1.9234613901630837e-14\n",
      "Epoch #41 Loss: 5.4761750689635846e-14\n",
      "Epoch #42 Loss: 6.800116025829084e-15\n",
      "Epoch #43 Loss: 3.522182545623309e-14\n",
      "Epoch #44 Loss: 6.541989172603735e-14\n",
      "Epoch #45 Loss: 2.3675506000131463e-14\n",
      "Epoch #46 Loss: 1.124100812432971e-14\n",
      "Epoch #47 Loss: 1.0352829704629585e-14\n",
      "Epoch #48 Loss: 1.3905543383430086e-14\n"
     ]
    }
   ],
   "source": [
    "input_rows = 2\n",
    "num_hidden_nodes = 2\n",
    "num_epochs = 48\n",
    "learning_rate = .1\n",
    "reg_const = 0\n",
    "batch_size = 4\n",
    "\n",
    "network = Network(input_rows, num_hidden_nodes)\n",
    "network.train(x_train, y_train, num_epochs, learning_rate, reg_const, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Network layer's weight matrices equal ~[[0,-1],[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1956e-08, -1.0000e+00],\n",
       "        [ 1.0000e+00,  8.9407e-08]], device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.layer2.W.output @ network.layer1.W.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing L2 loss: 0.0012183780781924725\n"
     ]
    }
   ],
   "source": [
    "network.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network with best parameters found :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss: 0.02058352343738079\n",
      "Epoch #2 Loss: 0.0004995372146368027\n",
      "Epoch #3 Loss: 0.00022733266814611852\n",
      "Epoch #4 Loss: 0.00026231163064949214\n",
      "Epoch #5 Loss: 2.4408750505244825e-06\n",
      "Epoch #6 Loss: 1.4741344500635023e-07\n",
      "Epoch #7 Loss: 4.4853010194856324e-14\n",
      "Epoch #8 Loss: 1.0169642905566434e-13\n",
      "Epoch #9 Loss: 1.496580637194711e-13\n",
      "Epoch #10 Loss: 4.3565151486291143e-13\n",
      "Epoch #11 Loss: 7.038813976123492e-13\n",
      "Epoch #12 Loss: 2.935429677108914e-13\n",
      "Epoch #13 Loss: 1.567634910770721e-13\n",
      "Epoch #14 Loss: 1.283417816466681e-13\n",
      "Epoch #15 Loss: 1.0169642905566434e-13\n",
      "Epoch #16 Loss: 2.1760371282653068e-14\n",
      "Epoch #17 Loss: 2.509104035652854e-13\n",
      "Epoch #18 Loss: 7.038813976123492e-13\n",
      "Epoch #19 Loss: 8.180567334648003e-12\n",
      "Epoch #20 Loss: 5.1958437552457326e-14\n"
     ]
    }
   ],
   "source": [
    "input_rows = 2\n",
    "num_hidden_nodes = 3\n",
    "num_epochs = 20\n",
    "learning_rate = .1\n",
    "reg_const = 0\n",
    "batch_size = 1\n",
    "\n",
    "network = Network(input_rows, num_hidden_nodes)\n",
    "network.train(x_train, y_train, num_epochs, learning_rate, reg_const, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Network layer's weight matrices equal ~[[0,-1],[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6391e-07, -1.0000e+00],\n",
       "        [ 1.0000e+00,  5.5879e-08]], device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.layer2.W.output @ network.layer1.W.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing L2 loss: 0.0006278472719714046\n"
     ]
    }
   ],
   "source": [
    "network.test(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
