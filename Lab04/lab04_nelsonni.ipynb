{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Lab 4: Tuning your autograd training loop for Fashion-MNIST\n",
    "### Author: Nigel Nelson\n",
    "### Course: CS-3450\n",
    "### Date: 4/3/2022\n",
    "---\n",
    "---\n",
    "\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import warnings\n",
    "import os.path\n",
    "import nvidia_smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Provided Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000\n",
    "\n",
    "# For fashion-MNIST and similar problems\n",
    "DATA_ROOT = '/../../data/cs3450/data/'\n",
    "FASHION_MNIST_TRAINING = '/../../data/cs3450/data/fashion_mnist_flattened_training.npz'\n",
    "FASHION_MNIST_TESTING = '/../../data/cs3450/data/fashion_mnist_flattened_testing.npz'\n",
    "CIFAR10_TRAINING = '/../../data/cs3450/data/cifar10_flattened_training.npz'\n",
    "CIFAR10_TESTING = '/../../data/cs3450/data/cifar10_flattened_testing.npz'\n",
    "CIFAR100_TRAINING = '/../../data/cs3450/data/cifar100_flattened_training.npz'\n",
    "CIFAR100_TESTING = '/../../data/cs3450/data/cifar100_flattened_testing.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
    "       https://d2l.ai/chapter_deep-learning-computation/use-gpu.html\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "DEVICE=try_gpu()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is the square example that we looked at in class.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_dataset_flattened(train=True,dataset='Fashion-MNIST',download=False):\n",
    "    \"\"\"\n",
    "    :param train: True for training, False for testing\n",
    "    :param dataset: 'Fashion-MNIST', 'CIFAR-10', or 'CIFAR-100'\n",
    "    :param download: True to download. Keep to false afterwords to avoid unneeded downloads.\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    if dataset == 'Fashion-MNIST':\n",
    "        if train:\n",
    "            path = FASHION_MNIST_TRAINING\n",
    "        else:\n",
    "            path = FASHION_MNIST_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-10':\n",
    "        if train:\n",
    "            path = CIFAR10_TRAINING\n",
    "        else:\n",
    "            path = CIFAR10_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-100':\n",
    "        if train:\n",
    "            path = CIFAR100_TRAINING\n",
    "        else:\n",
    "            path = CIFAR100_TESTING\n",
    "        num_labels = 100\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: '+str(dataset))\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading cached flattened data for',dataset,'training' if train else 'testing')\n",
    "        data = np.load(path)\n",
    "        x = torch.tensor(data['x'],dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'],dtype=torch.float32)\n",
    "        pass\n",
    "    else:\n",
    "        class ToTorch(object):\n",
    "            \"\"\"Like ToTensor, only to a numpy array\"\"\"\n",
    "\n",
    "            def __call__(self, pic):\n",
    "                return torchvision.transforms.functional.to_tensor(pic)\n",
    "\n",
    "        if dataset == 'Fashion-MNIST':\n",
    "            data = torchvision.datasets.FashionMNIST(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            data = torchvision.datasets.CIFAR10(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-100':\n",
    "            data = torchvision.datasets.CIFAR100(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        else:\n",
    "            raise ValueError('This code should be unreachable because of a previous check.')\n",
    "        x = torch.zeros((len(data[0][0].flatten()), len(data)),dtype=torch.float32)\n",
    "        for index, image in enumerate(data):\n",
    "            x[:, index] = data[index][0].flatten()\n",
    "        labels = torch.tensor([sample[1] for sample in data])\n",
    "        y = torch.zeros((num_labels, len(labels)), dtype=torch.float32)\n",
    "        y[labels, torch.arange(len(labels))] = 1\n",
    "        np.savez(path, x=x.detach().numpy(), y=y.detach().numpy())\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for CIFAR-10 training\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'Fashion-MNIST'\n",
    "dataset = 'CIFAR-10'\n",
    "# dataset = 'CIFAR-100'\n",
    "\n",
    "#x_train, y_train = create_linear_training_data()\n",
    "#x_train, y_train = create_folded_training_data()\n",
    "#points_train, insideness_train = create_square()\n",
    "x_train, y_train = load_dataset_flattened(train=True, dataset=dataset, download=False)\n",
    "\n",
    "# Move selected datasets to GPU\n",
    "x_train = x_train.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 50000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for CIFAR-10 testing\n"
     ]
    }
   ],
   "source": [
    "#x_test, y_test = create_linear_training_data()\n",
    "x_test, y_test = load_dataset_flattened(train=False, dataset=dataset, download=False)\n",
    "\n",
    "# Move the selected datasets to the GPU\n",
    "x_test = x_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6196, 0.9216, 0.6196,  ..., 0.0784, 0.0980, 0.2863],\n",
       "        [0.6235, 0.9059, 0.6196,  ..., 0.0745, 0.0588, 0.3843],\n",
       "        [0.6471, 0.9098, 0.5451,  ..., 0.0588, 0.0902, 0.3882],\n",
       "        ...,\n",
       "        [0.4863, 0.6980, 0.0314,  ..., 0.1961, 0.3137, 0.3686],\n",
       "        [0.5059, 0.7490, 0.0118,  ..., 0.2078, 0.3176, 0.2275],\n",
       "        [0.4314, 0.7804, 0.0275,  ..., 0.1843, 0.3137, 0.1020]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Backpropagation with Autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Responsible for modeling a single matrix in an Input\n",
    "    \"\"\"\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        :param output_shape (tuple): the shape of the output array.  When this is a single number,\n",
    "        it gives the number of output neurons. When this is an array, it gives the dimensions \n",
    "        of the array of output neurons.\n",
    "        \"\"\"\n",
    "        if not isinstance(output_shape, tuple):\n",
    "            output_shape = (output_shape,)\n",
    "            \n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        \n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    Responsible for modeling a single matrix in a Linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        :param output_shape (tuple): the shape of the output array. Passed to parent's initializer\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, output_shape)\n",
    "\n",
    "    def set(self, value):\n",
    "        \"\"\"\n",
    "        :param value: Value of the matrix. If the shape of the matrix doesn't meet the expectations\n",
    "        of the Input instance, an assertion error is raised\n",
    "        \"\"\"\n",
    "        assert self.output_shape == value.shape\n",
    "        self.output = value\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearReLU(Layer):\n",
    "    \"\"\"\n",
    "    Class responsible for modeling a Linear Layer with a ReLU activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, x, W, b):\n",
    "        \"\"\"\n",
    "        :param x: The input matrix of the layer\n",
    "        :param W: The weight matrix of the layer\n",
    "        :param b: The biase matrix of the layer. If this doesn't equal the Input's expected shape,\n",
    "        an assertion error is raised\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, b.output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        :param x: The values to perform the ReLU activation function on\n",
    "        \"\"\"\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Sets the layer's output based on the outputs of the layers that feed into it after applying the\n",
    "        ReLU activation function\n",
    "        \"\"\"\n",
    "        self.output = self.ReLU((self.W.output @ self.x.output) + self.b.output)\n",
    "        self.output.retain_grad()\n",
    "   \n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Class responsible for modeling a Linear Layer without an activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, x, W, b):\n",
    "        \"\"\"\n",
    "        :param x: The input matrix of the layer\n",
    "        :param W: The weight matrix of the layer\n",
    "        :param b: The biase matrix of the layer. If this doesn't equal the Input's expected shape,\n",
    "        an assertion error is raised\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, b.output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Sets the layer's output based on the outputs of the layers that feed into it\n",
    "        \"\"\"\n",
    "        self.output = (self.W.output @ self.x.output) + self.b.output\n",
    "        self.output.retain_grad()\n",
    "        \n",
    "class Network:\n",
    "    \"\"\"\n",
    "    Class responsible for defining the behavior of a simple Neural Network with a single hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_rows, num_hidden_nodes, num_classes, dtype=torch.float32, device=torch.device('cuda:0')):\n",
    "        \"\"\"\n",
    "        :param input_rows: The number of rows expected in the input of the network\n",
    "        :param num_hidden_nodes: The number of nodes in the hidden layer desired\n",
    "        :param num_classes: The number of expected outputs\n",
    "        :param dtype: The data type to be used with the PyTorch tensors\n",
    "        :param device: The device desired to be used with the PyTorch tensors\n",
    "        \"\"\"\n",
    "        # Define weights and bias matrices for input -> hidden layer\n",
    "        W = torch.rand((num_hidden_nodes, input_rows), dtype=dtype, device=device, requires_grad=True)\n",
    "        W.data *= 0.1\n",
    "        b1 = torch.zeros((num_hidden_nodes,1), dtype=dtype, device=device, requires_grad=True)\n",
    "        \n",
    "        # Define weights and bias matrices for hidden layer -> ouput\n",
    "        M = torch.rand((num_classes ,num_hidden_nodes), dtype=dtype, device=device, requires_grad=True)\n",
    "        M.data *= 0.1\n",
    "        b2 = torch.zeros((num_classes, 1), dtype=dtype, device=device, requires_grad=True)\n",
    "\n",
    "        # Create Input instances for all matrices\n",
    "        W_layer = Input((num_hidden_nodes, input_rows))\n",
    "        W_layer.set(W)\n",
    "        b1_layer = Input((num_hidden_nodes,1))\n",
    "        b1_layer.set(b1)\n",
    "        M_layer = Input((num_classes,num_hidden_nodes))\n",
    "        M_layer.set(M)\n",
    "        b2_layer = Input((num_classes,1))\n",
    "        b2_layer.set(b2)\n",
    "\n",
    "        # Create 1st layer with ReLU activation function\n",
    "        x1_layer = Input(x_train.shape[0])\n",
    "        linear_layer1 = LinearReLU(x1_layer, W_layer, b1_layer)\n",
    "        \n",
    "        # Create 2nd layer without activation function\n",
    "        x2_layer = Input(b1_layer.output.shape[0])\n",
    "        linear_layer2 = Linear(x2_layer, M_layer, b2_layer)\n",
    "        \n",
    "        # Assign class variables\n",
    "        self.layer1 = linear_layer1\n",
    "        self.layer2 = linear_layer2\n",
    "    \n",
    "    \n",
    "    def L2(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Returns the L2 loss of the supplied args\n",
    "        :param actual: The true values\n",
    "        :param predicted: The predicted values\n",
    "        \"\"\"\n",
    "        return ((actual - predicted)**2).mean()\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Responsible for calculating the softmax output of a provided network layer\n",
    "        :param X: The output of a provided network layer\n",
    "        \"\"\"\n",
    "        e = torch.exp(X - torch.max(X))\n",
    "        return e / e.sum()\n",
    "    \n",
    "    def cross_entropy(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss for predicted vs. the actual values\n",
    "        :param actual: The true values\n",
    "        :param predicted: The predicted values\n",
    "        \"\"\"\n",
    "        soft_max = self.softmax(predicted)\n",
    "        #Small constant such that soft_max + epsilon is never 0\n",
    "        epsilon = 1*10**-7\n",
    "        L = (actual * torch.log(soft_max + epsilon)).sum()\n",
    "        return -1*L, soft_max\n",
    "        \n",
    "    def train(self, x_train, y_train, num_epochs, learning_rate, reg_const, batch_size):\n",
    "        \"\"\"\n",
    "        Method responsible for training the Neural Network\n",
    "        :param x_train: The X training data\n",
    "        :param y_train: The y training labels\n",
    "        :param num_epochs: Number of epochs to train for\n",
    "        :param learning_rate: The rate at which parameters are adjusted\n",
    "        :param reg_const: The regularization constant that scales the regularization term\n",
    "        :param batch_size: The batch size used for training\n",
    "        \"\"\"\n",
    "        # Adjust the x matrices according to the batch size\n",
    "        self.layer1.x = Input((x_train.shape[0], batch_size))\n",
    "        self.layer2.x = Input((self.layer1.b.output.shape[0], batch_size))\n",
    "        \n",
    "        accuracies = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i in range(x_train.shape[1]//batch_size):\n",
    "                \n",
    "                num_samples += batch_size\n",
    "                \n",
    "                # Get the correct locations to reference in the training and testing sets\n",
    "                start_idx = i*batch_size\n",
    "                end_idx = i*batch_size + batch_size\n",
    "\n",
    "                # Populate the x matrix with the training samples in this batch\n",
    "                self.layer1.x.set(x_train[:, start_idx : end_idx].reshape(x_train.shape[0], batch_size))\n",
    "                self.layer1.forward()\n",
    "                self.layer2.x.set(self.layer1.output)\n",
    "                self.layer2.forward()\n",
    "                \n",
    "                true_labels = (y_train[:, start_idx : end_idx]).reshape(y_train.shape[0], batch_size)\n",
    "\n",
    "                # Calculate the Cross entropy loss using the output of layer 2 and the associated samples in\n",
    "                # y_train\n",
    "                loss, soft_max = self.cross_entropy(true_labels, self.layer2.output)\n",
    "                \n",
    "                # Calculate the number of correct predictions the network made\n",
    "                num_correct += (torch.argmax(true_labels, dim=0) == torch.argmax(soft_max, dim=0)).sum().item()\n",
    "                accuracy = num_correct / num_samples\n",
    "                \n",
    "                # Calculate the regularization term\n",
    "                s1 = (self.layer1.W.output**2).sum()\n",
    "                s2 = (self.layer2.W.output**2).sum()\n",
    "                reg = reg_const*(s1 + s2)\n",
    "\n",
    "                # Calculate the final cost term\n",
    "                cost = loss + reg\n",
    "\n",
    "                # Compute backpropagation with Autograd\n",
    "                cost.backward()\n",
    "            \n",
    "                # Used to update parameters inplace\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Adjust parameters according to gradients and the learning rate\n",
    "                    self.layer1.W.output -= learning_rate * self.layer1.W.output.grad\n",
    "                    self.layer1.b.output -= learning_rate * self.layer1.b.output.grad\n",
    "                    self.layer2.W.output -= learning_rate * self.layer2.W.output.grad\n",
    "                    self.layer2.b.output -= learning_rate * self.layer2.b.output.grad\n",
    "\n",
    "                    # Zero the gradients\n",
    "                    self.layer1.W.output.grad.zero_()\n",
    "                    self.layer1.b.output.grad.zero_()\n",
    "                    self.layer2.W.output.grad.zero_()\n",
    "                    self.layer2.b.output.grad.zero_()\n",
    "                    \n",
    "            print(f'Epoch #{epoch + 1} Accuracy: {accuracy}, Loss: {loss.item()}')\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            # Get GPU utilization rates\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            utilization = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
    "            print(f'gpu-usage: {utilization.gpu}%, gpu-mem-usage: {utilization.memory}%')\n",
    "            \n",
    "        return accuracies\n",
    "    \n",
    "    def test(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Method responsible for testing the Neural Network after it has been trained\n",
    "        :param x_train: The X training data\n",
    "        :param y_train: The training labels\n",
    "        \"\"\"\n",
    "        # Recalibrate the 1st x layer according to the shape of the testing data\n",
    "        self.layer1.x = Input(x_test.shape)\n",
    "        self.layer1.x.set(x_test)\n",
    "        \n",
    "        self.layer1.forward()\n",
    "        \n",
    "        # Recalibrate the 2nd x layer according to the shape of the output of the 1st layer\n",
    "        self.layer2.x = Input(self.layer1.output.shape)\n",
    "        self.layer2.x.set(self.layer1.output)\n",
    "\n",
    "        self.layer2.forward()\n",
    "        \n",
    "        true_labels = torch.argmax(y_test, dim=0)\n",
    "        predicted = torch.argmax(self.softmax(self.layer2.output), dim=0)\n",
    "        \n",
    "        num_correct = (predicted == true_labels).sum().item()\n",
    "        total_samples = y_test.shape[1]\n",
    "        accuracy = num_correct / total_samples\n",
    "    \n",
    "        print(f'Testing Accuracy: {accuracy}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input rows: 3072\n",
      "Num hidden nodes: 24\n",
      "Epochs: 30\n",
      "Learning Rate: 0.001\n",
      "Reg_const: 0\n",
      "Batch size: 4\n",
      "\n",
      "Epoch #1 Accuracy: 0.09724, Loss: 64.47238159179688\n",
      "gpu-usage: 23%, gpu-mem-usage: 0%\n",
      "Epoch #2 Accuracy: 0.09894, Loss: 64.47238159179688\n",
      "gpu-usage: 18%, gpu-mem-usage: 0%\n",
      "Epoch #3 Accuracy: 0.09966, Loss: 48.354366302490234\n",
      "gpu-usage: 18%, gpu-mem-usage: 0%\n",
      "Epoch #4 Accuracy: 0.10008, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #5 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #6 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #7 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 22%, gpu-mem-usage: 0%\n",
      "Epoch #8 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #9 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #10 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #11 Accuracy: 0.1, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #12 Accuracy: 0.09982, Loss: 64.47238159179688\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #13 Accuracy: 0.23674, Loss: 13.207659721374512\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #14 Accuracy: 0.37194, Loss: 11.645803451538086\n",
      "gpu-usage: 20%, gpu-mem-usage: 0%\n",
      "Epoch #15 Accuracy: 0.40018, Loss: 11.5936861038208\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #16 Accuracy: 0.41432, Loss: 11.164941787719727\n",
      "gpu-usage: 20%, gpu-mem-usage: 0%\n",
      "Epoch #17 Accuracy: 0.42546, Loss: 11.285964012145996\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #18 Accuracy: 0.43274, Loss: 11.299300193786621\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #19 Accuracy: 0.44006, Loss: 11.345144271850586\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #20 Accuracy: 0.44498, Loss: 11.197125434875488\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #21 Accuracy: 0.44974, Loss: 11.45274543762207\n",
      "gpu-usage: 18%, gpu-mem-usage: 0%\n",
      "Epoch #22 Accuracy: 0.45388, Loss: 11.381828308105469\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #23 Accuracy: 0.45652, Loss: 11.50838851928711\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #24 Accuracy: 0.4585, Loss: 11.640085220336914\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #25 Accuracy: 0.46298, Loss: 11.577932357788086\n",
      "gpu-usage: 18%, gpu-mem-usage: 0%\n",
      "Epoch #26 Accuracy: 0.46422, Loss: 11.78243637084961\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #27 Accuracy: 0.46688, Loss: 11.777826309204102\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "Epoch #28 Accuracy: 0.46856, Loss: 12.134273529052734\n",
      "gpu-usage: 18%, gpu-mem-usage: 0%\n",
      "Epoch #29 Accuracy: 0.46976, Loss: 12.151333808898926\n",
      "gpu-usage: 18%, gpu-mem-usage: 0%\n",
      "Epoch #30 Accuracy: 0.472, Loss: 11.944917678833008\n",
      "gpu-usage: 19%, gpu-mem-usage: 0%\n",
      "CPU times: user 17min 55s, sys: 15.4 s, total: 18min 10s\n",
      "Wall time: 18min 17s\n",
      "Testing Accuracy: 0.4549\n"
     ]
    }
   ],
   "source": [
    "input_rows = 3072\n",
    "num_hidden_nodes = 24\n",
    "num_epochs = 30\n",
    "learning_rate = .001\n",
    "reg_const = 0\n",
    "batch_size = 4\n",
    "num_classes = 10\n",
    "\n",
    "print(f'Input rows: {input_rows}')\n",
    "print(f'Num hidden nodes: {num_hidden_nodes}')\n",
    "print(f'Epochs: {num_epochs}')\n",
    "print(f'Learning Rate: {learning_rate}')\n",
    "print(f'Reg_const: {reg_const}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print()\n",
    "\n",
    "network = Network(input_rows, num_hidden_nodes, num_classes)\n",
    "%time accuracies = network.train(x_train, y_train, num_epochs, learning_rate, reg_const, batch_size)\n",
    "network.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Records of Network Tuning for Fashion-MNIST:\n",
    "\n",
    "| Experiment Number | Learning Rate | Batch Size | Epochs | Regularization Constant | Hidden Nodes | Parameters | Estimated Runtime | Actual Runtime | Estimated Memory Usage | GPU usage | Number layers | Training Accuracy | Testing Accuracy |\n",
    "| ----------------- | ------------- | ---------- | ------ | ----------------------- | ------------ | ---------- | ----------------- | -------------- | ---------------------- | --------- | ------------- | ----------------- | ---------------- |\n",
    "| 1                 | 0.001         | 1          | 12     | 0                       | 24           | 38,180     | N/A               | 30min 48s      | 152.72 kB              | 18%       | 1             | 0.87555           | 0.8574           |\n",
    "| 2                 | 0.001         | 4          | 20     | 0                       | 24           | 38,180     | 11min 30s         | 13min 6s       | 152.72 kB              | 20%       | 1             | 0.87995           | 0.8593           |\n",
    "| 3                 | 0.01          | 4          | 20     | 0                       | 24           | 38,180     | 13min             | 13min 1s       | 152.72 kB              | 20%       | 1             | 0.86957           | 0.8452           |\n",
    "| 4                 | 0.001         | 4          | 20     | 0                       | 48           | 76,340     | 13min             | 13min 47s      | 305.36 kB              | 21%       | 1             | 0.88975           | 0.8703           |\n",
    "| 5                 | 0.001         | 4          | 20     | 0                       | 128          | 203,540    | 13min             | 13min 18s      | 814.16 kB              | 22%       | 1             | 0.89756           | 0.8747           |\n",
    "| 6                 | 0.001         | 16         | 30     | 0                       | 128          | 203,540    | 6min 30s          | 4min 28s       | 814.16 kB              | 26%       | 1             | 0.9088            | 0.8809           |\n",
    "| 7                 | 0.001         | 16         | 60     | 0                       | 128          | 203,540    | 9min              | 10min 18s      | 814.16 kB              | 26%       | 1             | 0.93446           | 0.8887           |\n",
    "| 8                 | 0.001         | 16         | 60     | 0                       | 256          | 407,060    | 10min             | 10min 22s      | 1.62 mB                | 26%       | 1             | 0.93443           | 0.8875           |\n",
    "| 9                 | 0.001         | 32         | 60     | 0                       | 256          | 407,060    | 5min              | 5min 18s       | 1.62 mB                | 26%       | 1             | 0.90865           | 0.8763           |\n",
    "| 10                | 0.001             | 64         | 120    | 0.001                   | 256          | 407,060    | 5min              | 5min 17s       | 1.62 mB                | 30%       | 1             | 0.93811           | 0.8873           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification for Experiments:\n",
    "\n",
    "1. The hyperparameters for the first training session were chosen based on trial and error from the early stages of this lab when my network was still reluctant to learn. I chose a batch size of 1 to keep debugging simple, a learning rate of 0.001 as this was the learning rate I found allowed my network to learn at a reasonable rate, and 24 hidden nodes as I learned early on that using more than 100 made it very difficult for my model to learn.\n",
    "2. After the first training session, I wanted to increase the efficiency of the training so I increased my batch size to 4. With this change I knew that this would mean my network would have its parameters updated 1/4 as many times as the first run, so to increase the number of opportunities to learn I decided to train for 20 epochs as it seemed like a large enough time train, but not too large to where training would take an hour or more.\n",
    "3. With the benefit seen in accuracy and GPU usage from increasing the training epochs and batch size, I was curious to see if increasing the learning rate would allow my network to learn more at a faster pace. Following the learning rate conventions I've been taught at MSOE, I scaled my learning rate by a factor of 10 and set it to 0.01.\n",
    "4. After learning that a larger learning rate had a negative impact on my network's performance, I returned the learning rate to my previous rate of 0.001. However, with the intuition that doubling the number of hidden nodes in my hidden layer would double the amount of information my network could store to properly learn the nuances of the dataset, I set the number of hidden nodes to 48 to see if this would increase the accuracy of my network.\n",
    "5. From experiment #4, I saw that there may be a positive correlation with the number of hidden nodes and the accuracy of the network, so to again test my intuition expressed in the previous experiment's justification, I doubled the number of hidden nodes to 128.\n",
    "6. After seeing a benefit in the accuracy of my model with previous hyperparameter adjustments, I was now interested to increase my GPU utilization as 20% usage seemed quite inefficient. Knowing that increasing the batch size would allow for greater parallelization, which would in turn increase the amount of work to be done by the GPU, I increased the batch size by a factor of 4 to 16. With 60,000 samples in an epoch, and a batch size of 4, my network was adjusting its parameters 300,000 times in my previous training sessions, however, with a batch size of 16, my network would now only update its hyperparameters 75,000 times. To allow my network more opportunities to learn without having too long of a training time, I increased the number of epochs by 10 to give my network an additional 37,500 opportunities to update its parameters.\n",
    "7. With a slight increase seen in my network with the increased batch size and number of epochs, I wanted to see if doubling the number of opportunities for my network to adjust its parameters would have a profound impact on the resulting accuracy of the model. So, I set the number of epochs to 60 epochs, and with a batch size of 16, I gave my network an additional 112,500 opportunities to adjust its parameters.\n",
    "8. Once again I was interested to see if increasing the number of hidden nodes would allow my network to learn more intricate nuances in the Fashion-MNIST dataset. With this in mind, I doubled the number of hidden nodes to 256 so that in theory, I would double the amount of information that my network could store on the dataset's patterns.\n",
    "9. With low GPU usage, I knew that increasing the batch size would allow for a higher degree of parallelization, which would take advantage of the GPU's resources. So, in the hopes that doubling the batch size would double the amount of GPU usage, I set the batch size to 32.\n",
    "10. After never using regularization, I was curious to see how this would impact the performance of the network, so based on successful trials in lab 3, I set the regularization constant to 0.001. In addition, I was still not satisfied with my low GPU usage, so I again doubled the batch size to 64 to hopefully increase my GPU usage. Lastly, to offset the fact that my network would now have half the opportunities to update its parameters, I doubled the number of epochs I would train for to give my network the same number opportunities to update its parameters as in the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Records of Network Tuning for CIFAR-10:\n",
    "\n",
    "| Experiment Number | Learning Rate | Batch Size | Epochs | Regularization Constant | Hidden Nodes | Parameters | Estimated Runtime | Actual Runtime | Estimated Memory Usage | GPU usage | Number layers | Training Accuracy | Testing Accuracy |\n",
    "| ----------------- | ------------- | ---------- | ------ | ----------------------- | ------------ | ---------- | ----------------- | -------------- | ---------------------- | --------- | ------------- | ----------------- | ---------------- |\n",
    "| 1                 | 0.001         | 4          | 30     | 0                       | 12           | 74,132     | N/A               | 17min 45s      | 296.528 kB             | 18%       | 1             | 0.4404            | 0.4153           |\n",
    "| 2                 | 0.001         | 4          | 30     | 0                       | 24           | 148,004    | 18min             | 18min 15s      |  592.016 kB            | 19%       | 1             | 0.472            | 0.4549           |\n",
    "| 3                 | 0.001         | 4          | 30     | 0                       | 128          | 789,268    | 18min 30s         | 18min 26s      |  3.157072 mB           | 22%       | 1             | 0.1261            | 0.0999           |\n",
    "| 4                 | 0.001         | 4          | 30     | 0                       | 48           | 295,988    | 18min             | 18min 6s       |  1.183952 mB           | 21%       | 1             | 0.44126           | 0.4355           |\n",
    "| 5                 | 0.001         | 8          | 60     | 0                       | 48           | 295,988    | 18min             | 18min 24s      |  1.183952 mB           | 20%       | 1             | 0.45812           | 0.4344           |\n",
    "| 6                 | 0.001         | 8          | 60     | 0                       | 64           | 394,644    | 18min 30s         | 18min 30s      |  1.578576 mB           | 21%       | 1             | 0.42108           | 0.4156           |\n",
    "| 7                 | 0.001         | 16         | 120    | 0                       | 64           | 394,644    | 18min 30s         | 18min 44s      |  1.578576 mB           | 20%       | 1             | 0.40286           | 0.4043           |\n",
    "| 8                 | 0.005         | 16         | 120    | 0                       | 48           | 295,988    | 18min 30s         | 18min 7s       |  1.183952 mB           | 20%       | 1             | 0.1006            | 0.0986           |\n",
    "| 9                 | 0.001         | 32         | 240    | 0                       | 48           | 295,988    | 18min             | 18min 26s      |  1.183952 mB           | 20%       | 1             | 0.48897           | 0.4213           |\n",
    "| 10                | 0.0001        | 32         | 240    | 0.001                   | 128          | 789,268    | 18 30s            | 18min 19s      |  3.157072 mB           | 23%       | 1             | 0.10113           | 0.1              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curve for Most Accurate Hyperparameters on CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Accuracy vs. Epoch #')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(num_epochs), accuracies)\n",
    "plt.xlabel('Training Epoch #')\n",
    "plt.ylabel('Training Epoch Accuracy')\n",
    "plt.title('Training Accuracy vs. Epoch #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "- From the experiments I ran in this lab, I learned many lessons about tuning the hyperparameters of a neural network. The most general lesson that I learned, is that a good rule of thumb to remember is that just about every hyperparameter has a parabolic relationship with the accuracy of the network. This means that while increasing a hyperparameter may in turn cause higher network accuracy, eventually increases in this same hyperparameter will cease to improve the accuracy of the network, and will even likely decrease the accuracy instead. More specifically, I learned that having too many hidden nodes makes it very difficult for a network to train, but having too few starves the model of complexity that allows for greater accuracy. In addition, I learned increasing the batch size will general increase the amount of GPU usage, likely due to the increased degree of parallelization. I also learned that while increasing epochs does intuitively give the network more opportunities to learn and thereby increase the accuracy of the model, it also gives the network more opportunities to go beyond generalizing the dataset and instead begin to memorize the dataset and overfit. Lastly, I learned that batch size and the number of training epochs are essentially linearly correlated with training time, where doubling the epochs will double the training time and doubling batch size will halve the training time.\n",
    "- Using these lessons that were learned through trial and error in this lab, I was ultimately able to achieve a maximum testing accuracy of on Fashion-MNIST of **0.8887**. This accuracy was achieved by training for 60 epochs, with a learning rate of 0.001, a batch size of 16, no regularization, and 128 nodes in a single hidden layer. In addition, I was able to achieve a maximum accuracy of **0.4549** on the CIFAR-10 dataset. This was accomplished by training for 30 epochs, with a learning rate of 0.001, a batch size of 4, no regularization, and 24 nodes in a single hidden layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
