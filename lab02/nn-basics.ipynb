{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Basics of Feed-Forward Neural Networks\n",
    "## Author: Nigel Nelson\n",
    "\n",
    "In this lab, we will start to create a feed-forward neural network from scratch.\n",
    "We begin with the very basic computational unit, a perceptron,\n",
    "and then we will add more layers and increase the complexity of our network. Along the way, we will learn how a perceptron works, the benefits of adding more layers, the kind of transformations necessary for learning complex features and relationships from data, and why an object-oriented paradigm is useful for easier management of our neural network framework.\n",
    "\n",
    "We will implement everything from scratch in Python using helpful\n",
    "libraries such as NumPy and PyTorch (without using the autograd feature of PyTorch). The purpose of this lab and the following lab\n",
    "series is to learn how neural networks work starting from the most basic\n",
    "computational units and proceeding to deeper and more networks. This will help us better understand how other popular deep learning frameworks, such as PyTorch, work underneath. You should be able to easily understand and implement everything in this lab. If you are having trouble consult with your instructors as the next lab series will assume a perfect understanding of the basic feed-forward neural network material.\n",
    "\n",
    "The recommended Python version for this implementation is 3.7. Recommended reading: sections 4.1 and 4.2 of the book (https://www.d2l.ai/chapter_multilayer-perceptrons/index.html).\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A perceptron or artificial neuron is the most basic processing unit of feed-forward neural networks. A perceptron can be modeled as a single-layer neural network with an input vector $\\mathbf{x} \\in \\mathbb{R}^n$, a bias $b$, a vector of trainable weights $\\mathbf{w} \\in \\mathbb{R}^n$, and an output unit $y$. Given the input $\\mathbf{x}$, the output $y$ is computed by an activation function $f(\\cdot)$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i w_i\\right) + b \\right) = f(\\mathbf{w}^\\intercal \\mathbf{x} + b)\\,,\n",
    "\\end{equation}\n",
    "where $\\Theta = \\{\\mathbf{w}, b\\}$ represents the trainable parameter set. \n",
    "\n",
    "The figure below shows a schematic view of a single output perceptron. Each input value $x_i$ is multiplied by a weight factor $w_i$. The weighted sum added to the bias is then passed through an activation function to obtain the output, $y$.\n",
    "\n",
    "![MLP example](img/perceptron.png)\n",
    " \n",
    "The vector $\\mathbf{x}$ represents one sample of our data and each element $x_i$ represents a feature. Thus, $\\mathbf{x}$ is often referred to as a feature vector. These features can represent different measurements depending on the application. For example, if we are trying to predict if a patient is at high risk of cardiac disease then each element of $\\mathbf{x}$ might contain vital signs such as diastolic and systolic blood pressure, heart rate, blood sugar levels, etc. In another application where we are trying to predict if a tissue biopsy is cancerous or not using mid-infrared imaging then each element of $\\mathbf{x}$ can represent the amount of mid-infrared light absorbed at a particular wavelength. The output $y$ in the applications above could contain values of $0$ or $1$, indicating if the patient is at high risk of cardiac disease or if the tissue biopsy is cancerous or not.\n",
    " \n",
    "Now, let us begin implementing our first artificial neuron.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's assume that our feature vector contains measurements of body temperature pressure, pulse oximeter reading, and presence of cough or not. Then for a 'healthy' patient our input sample might look like $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\end{bmatrix}$. Let's say that we are trying to 'predict' the probability of a patient being positive with COVID-19 based on the above measurements.\n",
    "\n",
    "Each element of our input vector is associated with a unique weight. Let the vector of weights be $\\mathbf{w} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\end{bmatrix}$. Each artifical neuron is also associated with a unique bias. Let the bias be $b = 2.9$. Assuming a linear activation function write the code to produce and print the output $y$ given the above input vector $\\mathbf{x}$, weights $\\mathbf{w}$, and the bias $b$ using the above perceptron model. Do not use any NumPy or PyTorch functions. Use a Python variables for each mathematical variable and use Python lists for vectors.\n",
    "\n",
    "For the activation function, use ReLU. This can be computed as ```x * (x > 0)``` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [98.6, 95, 0]\n",
    "w = [0.03, 0.55, 0.88]\n",
    "b = 2.9\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the perceptron is 58.108000000000004\n"
     ]
    }
   ],
   "source": [
    "y = ReLU(sum([x*w for x,w in zip(x,w)]) + b)\n",
    "print(f'The output of the perceptron is {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 1: How many parameters does our simple model contain? Be specific.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: The model contains 4 parameters. This is because there is a weight vector, which contains 3 weight parameters, and a single bias parameter, resulting in 4 total parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 2: Recall that we were hoping to 'predict' the probability of a patient being positive with COVID-19. Does the output make sense? If not, elaborate on how you could fix it.</font>\n",
    "Ans: Due to the fact that we are attempting to predict the **probability** of a patient being positive with COVID-19, the output *~58.1* does not make complete sense. It is common place to have probabilities reported in the range of 0-1, where .2 would correspond to a 20% probability of an event occurring. In order to introduce this standard notation of probability, one could apply the Sigmoid function, which takes any input, and will map that input between the range of 0-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Multiple Outputs\n",
    "\n",
    "The perceptron model above has only one output. However, in most applications, we need multiple outputs. For example, in a classification problem, we would expect the model to output a vector $\\mathbf{y}$, where each $y_i$ represents the probability of a sample belonging to a particular class $i$. The figure below shows a schematic view of a multiple output feed-forward neural network. Each input value $x_i$ is multiplied by a weight factor $W_{ij}$, where $W_{ij}$ denotes a connection weight between the input node $x_i$ and the output node $y_j$. The weighted sum is added to the bias and then passed through an activation function to obtain the output, $y_j$.\n",
    "\n",
    "![Multi outout perceptron](img/multi-output-perceptron.png)\n",
    "\n",
    "Given an input $\\mathbf{x} \\in \\mathbb{R}^n$ this can be modeled as:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i W_{ij}\\right) + b_j\\right) = f(\\mathbf{w}_j^\\intercal \\mathbf{x} + b_j)\\,,\n",
    "\\end{equation}\n",
    "where the parameter set here is $\\Theta = \\{ \\mathbf{W} \\in \\mathbb{R}^{n \\times m}, \\mathbf{b} \\in \\mathbb{R}^m \\}$ and $\\mathbf{w}_j$ denotes the $j^{th}$ column of $\\mathbf{W}$. \n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "\n",
    "Let $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\\\ 1 \\end{bmatrix}$. Let the output vector $\\mathbf{y} \\in \\mathbb{R}^3$, i.e. consisting of $3$ outputs. Let the weights associated with each output node $y_i$ be $\\mathbf{w_1} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\\\0.73 \\end{bmatrix}$, $\\mathbf{w_2} = \\begin{bmatrix} 0.48 \\\\ 0.31 \\\\ 0.28 \\\\ -0.9 \\end{bmatrix}$, $\\mathbf{w_3} = \\begin{bmatrix} 0.77 \\\\ 0.54 \\\\ 0.37 \\\\ 0.44 \\end{bmatrix}$. Let the bias vector be $\\mathbf{b} = \\begin{bmatrix} 2.9 \\\\ 6.1 \\\\ 3.3 \\end{bmatrix}$. Note that a single bias is associated with each output node $y_i$.\n",
    "\n",
    "Given the above inputs write the code to print the output vector $\\mathbf{y}$.  Use a Python variables for each mathematical variable and use Python lists for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "x = [98.6, 95, 0 , 1]\n",
    "w1 = [0.03, 0.55, 0.88, 0.73]\n",
    "w2 = [0.48, 0.31, 0.28, -0.9]\n",
    "w3 = [0.77, 0.54, 0.37, 0.44]\n",
    "b = [2.9, 6.1, 3.3]\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for node y1 is 58.838\n",
      "Output for node y2 is 81.97799999999998\n",
      "Output for node y3 is 130.96200000000002\n"
     ]
    }
   ],
   "source": [
    "y1 = ReLU(sum([x*w1 for x,w1 in zip(x,w1)]) + b[0])\n",
    "y2 = ReLU(sum([x*w2 for x,w2 in zip(x,w2)]) + b[1])\n",
    "y3 = ReLU(sum([x*w3 for x,w3 in zip(x,w3)]) + b[2])\n",
    "\n",
    "print(f'Output for node y1 is {y1}')\n",
    "print(f'Output for node y2 is {y2}')\n",
    "print(f'Output for node y3 is {y3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Now that you understand how to do basic computations with a simple perceptron model manually, we will proceed to implement the same model above using matrix-vector operations utilizing PyTorch functions. Organizing the computations in matrix-vector format notation makes it simpler to understand and implement neural network models. \n",
    "\n",
    "Write the code to create the same output vector $\\mathbf{y}$ as above by expressing the above computations as matrix-vector multiplications and summation with a bias vector using code vectorization in PyTorch. You should get the same output as above, up to floating-point errors. Again use Python variables for the mathematical variables, but use PyTorch arrays for vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([98.6, 95, 0 , 1])\n",
    "W = torch.tensor([[0.03, 0.55, 0.88, 0.73],\n",
    "                 [0.48, 0.31, 0.28, -0.9],\n",
    "                 [0.77, 0.54, 0.37, 0.44]])\n",
    "b = torch.tensor([2.9, 6.1, 3.3])\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for node y1 is 58.8380012512207\n",
      "Output for node y2 is 81.97799682617188\n",
      "Output for node y3 is 130.96200561523438\n"
     ]
    }
   ],
   "source": [
    "y = ReLU((x @ W.T) + b)\n",
    "print(f'Output for node y1 is {y[0]}')\n",
    "print(f'Output for node y2 is {y[1]}')\n",
    "print(f'Output for node y3 is {y[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### <font color=blue>Question 3: Explain what each of the dimensions of the matrix of weights $\\mathbf{W}$ and the vector of biases $\\mathbf{b}$ represent?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: The dimensions of the weights matrix is 3X4, this is because there are 3 output nodes, each of which multiply the 4 input nodes by a unique weight vector. This is such that the first row of weights, corresponds to the weights for the first output node, the second row corresponds to weights for the second output node, ect. The bias vector has a dimension of 1X3 because there are 3 output nodes, and each one has a bias associated with it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 4: What is the total number of parameters for this model?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: This model has a weight matrix with a dimension of 3X4, which means it has 12 weight parameters. In addition, it has a bias vector containing 3 bias parameters. As such, this model has a total of 15 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "## More Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "A single-layer perceptron network still represents a linear classifier, even if we were to use nonlinear activation functions. This limitation can be overcome by multi-layer neural networks in combination with nonlinear activation functions, which introduce one or more 'hidden' layers between the input and output layers. Multi-layer neural networks are composed of several simple artificial neurons such that the output of one acts as the input of another. A multi-layer neural network can be represented by a composition function. For a two-layer network with only one output, the composition function can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(2)}\\left(\\sum_{k=1}^{h}W_{kj}^{(2)}*f^{(1)}\\left(\\left(\\sum_{i=1}^{n}W_{ik}^{(1)}*x_i \\right)+b_k^{(1)}\\right)+b_j^{(2)}\\right)\n",
    "\\end{equation}\n",
    "where $h$ is the number of units in the hidden layer and the set of unknown parameters is $\\Theta = \\{\\mathbf{W}^{(1)} \\in R^{n \\times h}, \\mathbf{W}^{(2)} \\in R^{h \\times 1}\\}$. In general, for $L - 1$ hidden layers the composition function, omitting the bias terms, can be written as\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(L)}\\left(\\sum_k W_{kj}^{L}*f^{L-1}\\left(\\sum_{l}W_{lk}^{L - 1}* f^{L - 2}\\left( \\cdots f^{1}\\left(\\sum_{i}W_{iz}^{1}*x_i \\right)\\right) \\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The figure below illustrates a feed-forward neural network composed of an input layer, a hidden layer, and an output layer. In this illustration, the multi-layer neural network has one input layer and one output unit. In most models, the number of hidden layers and output units is more than one.\n",
    "\n",
    "![Feed forward perceptron](img/feed-forward.png)\n",
    "\n",
    "We will now see how to add an additional layer to our model and then how to generalize to any number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add another layer we need another set of weights and biases. \n",
    "\n",
    "W_2 = torch.tensor([[-0.3, 0.66, 0.98],\n",
    "                   [0.58, -0.4, 0.38],\n",
    "                   [0.87, 0.69, -0.4]])\n",
    "\n",
    "b_2 = torch.tensor([3.9, 8.2, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 28
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for node y1 is tensor([168.6969,  59.3004,  56.1691])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write the code to print the output of a 2-layer feed-forward network using the previously computed output,\n",
    "# y, as input to the second layer \n",
    "y_1 = ReLU((y @ W_2.T) + b_2)\n",
    "\n",
    "print(f'Output for node y1 is {y_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "### <font color=blue>Question 5: Explain the dimensions of the weight matrix for the second layer with respect to the dimensions of the previous layer and the number of artificial neurons in the second layer. or Why are the dimensions of the weight matrix of the second layer 3x3?</font>\n",
    "\n",
    "Ans: The reason that the dimensions of the weight matrix for the second layer are 3X3 is because the output from the second layer is a 1X3 vector. As such, the weight matrix must be of dimension 3XN in order for matrix multiplication to be allowed and to properly receive the input from the 3 nodes. Also, due to the fact that 3 output nodes is still desired, the second layer weight matrix's dimensions must be 3X**3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers to Objects\n",
    "\n",
    "Now, we have a feed-forward model (with an input layer, one hidden layer, and one output layer with 3 outputs) capable of processing a batch of data. It would be cumbersome and redundant if we had to keep writing the same code for hundreds of layers. So, to make our code more modular, easier to manage, and less redundant we will represent layers using an object-oriented programming paradigm. Let's define classes for representing our layers.\n",
    "\n",
    "All layer objects should have an `output` instance attribute.  Use good object-oriented practices to avoid code duplication.  To initialize an instance attribute in Python, write `self.attribute_name = attribute_value` in the initializer (`__init__` method).  Don't mention the variable at the top of the class as we would usually do in Java -- this is how you define static attributes in Python.\n",
    "\n",
    "Rather than each layer taking PyTorch arrays as inputs, it should take `Layer`s as inputs, with each layer having its own name. For example, if your network would take $\\mathbf{x}$, $\\mathbf{W}$, and $\\mathbf{b}$ as inputs, you should have attributes `self.x`, `self.W`, and `self.b`.  Then, when you need the values of these inputs, go back and read the output of the previous layer.  For example, if your layer needs the value of $\\mathbf{W}$, you could read `self.W.output` to get it.\n",
    "\n",
    "Two more Python OO hints: (1) `class MyClass1(MyClass2)` is not a constructor call. It is specifying the inheritance relationship. The Java equivalent is `class MyClass1 extends MyClass2`. So you don't want to add arguments on this line. An easy mistake to make.  (2) You must use `self.` every time you access an instance variable in Python. This is how the language was designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the following classes.\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        TODO: Initialize instance attributes here.\n",
    "        :param output_shape (tuple): the shape of the output array.  When this isa single number,\n",
    "        it gives the number of output neurons. When this is an array, it gives the dimensions \n",
    "        of the array of output neurons.\n",
    "        \"\"\"\n",
    "        if not isinstance(output_shape, tuple):\n",
    "            output_shape = (output_shape,)\n",
    "            \n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "\n",
    "    def set(self,value):\n",
    "        \"\"\"\n",
    "        TODO: set the `output` of this array to have value `value`.\n",
    "        Raise an error if the size of value is unexpected. An `assert()` is fine for this.\n",
    "        \"\"\"\n",
    "        assert self.output_shape == value.shape\n",
    "        self.output = value\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, x, W, b):\n",
    "        \"\"\"\n",
    "        TODO: Accept any arguments specific to this child class.\n",
    "        \n",
    "        Raise an error if any of the argument's size do not match as you would expect.\n",
    "        \"\"\"\n",
    "        Layer.__init__(self, b.output_shape) # TODO: Pass along any arguments to the parent's initializer here.\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def ReLU(x):\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: Set this layer's output based on the outputs of the layers that feed into it.\n",
    "        \"\"\"\n",
    "        self.output = ReLU((self.x.output @ self.W.output.T) + self.b.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " output of hidden layer \n",
      " [ 58.838  81.978 130.962]\n"
     ]
    }
   ],
   "source": [
    "# This example uses your classes to test the output of the entire network.\n",
    "#\n",
    "# You may change this example to match your own implementation if you wish.\n",
    "x = [98.6, 95, 0 , 1]\n",
    "W = [[0.03, 0.55, 0.88, 0.73],\n",
    "    [0.48, 0.31, 0.28, -0.9],\n",
    "    [0.77, 0.54, 0.37, 0.44]]\n",
    "b = [2.9, 6.1, 3.3]\n",
    "\n",
    "x_layer = Input(4)\n",
    "x_layer.set(np.array(x))\n",
    "W_layer = Input((3, 4))\n",
    "W_layer.set(np.array(W))\n",
    "b_layer = Input(3)\n",
    "b_layer.set(np.array(b))\n",
    "linear_layer = Linear(x_layer, W_layer, b_layer)\n",
    "\n",
    "linear_layer.forward()\n",
    "print('\\n output of hidden layer \\n', linear_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this lab... except for the two **required** parting questions:\n",
    "\n",
    "### <font color=blue>Question 6: Summarize what you learned during this lab.\n",
    "\n",
    "Ans: In this lab I learned about the basics of creating a Neural Network from scratch. This began by first understanding the fundamental processing unit of a feed forward network, a perceptron. I learned that the equation needed to compute the output value of a perceptron is:\n",
    "\\begin{equation}\n",
    "y (\\mathbf{x}; \\Theta) = f\\left(\\left(\\sum_{i=1}^{n} x_i w_i\\right) + b \\right) = f(\\mathbf{w}^\\intercal \\mathbf{x} + b)\\,,\n",
    "\\end{equation}\n",
    "Which once understood makes coding a simple feedforward perceptron very easy if simple python lists are used for the variables. Through this verbose exercise I came to understand that all a feedforward perceptron does, or even a single neuron in a larger network does, is multiply each of its inputs by the corresponding weight, add the corresponding bias, sum the biased output together, and finally apply an activation function to that sum. With this verbose understanding, it made me appreciate matrix and vector libraries such as numpy or PyTorch much more. Another lesson I gained from this lab is how output dimensions are changed layer by layer using simple matrix multiplication. In addition, I learned that in order to string multiple feedforward layers together, one must nest multiple instances of the above function inside one another, where the previous layer in the sequence replaces the x variable with a complete function of its own. Lastly, I learned that using object oriented programming practices in order to code a multi-layer neural network reduces a great deal of redundance, makes the final result more intuitive, and is overall more robust than hand coding each layer as either a numpy array or a PyTorch Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 7: Describe what you liked about this lab *or* what could be improved. (Required.)\n",
    "\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I enjoyed that thoughtfulness that was put into this lab write up. It was a very well written notebook, and following along with this tutorial/lab was extremely easy because of this. I also appreciated the included mathematical equations as well as the pictures because at times the text was difficult to grasp but the included visuals would often clean up any misunderstandings. In addition, I liked the progression of difficulty of this lab, it made it so that lessons were taught at each step that could be applied to later questions/tasks. One of the only things that I think could be improved is the documentation and instructions given for the Layer, Input, and Linear classes. In hindsight it was not a very difficult implementation, but it was difficult to understand how the whole thing was intended to work together, and I spent a significant amount of time just trying to understand what should go where due to the vagueness of directions. I think that section could benefit from more hints, or even give more examples to run and the correct answers so that students know what to aim for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
